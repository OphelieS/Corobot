{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e11482-0b35-4483-928e-522801660947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from stop_words import get_stop_words\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\"\"\"\n",
    "- En utilisant la fonction `open` (et en précisant le paramètre `encoding` adéquat!), importez le texte dans une chaîne de caractères que vous nommerez `texte`.  \n",
    "- Procédez à quelques nettoyages : \n",
    "    - passez le texte en minuscules.  \n",
    "    - vérifiez s'il n'y pas d'acronyme pouvant fausser notre tokenization en phrases, si oui, remplacez-le par autre chose.  \n",
    "    - le virus est appelé covid-19 ou coronavirus, faites en sorte qu'un terme unique soit utilisé.   \n",
    "- Avec la fonction `nltk.sent_tokenize`, passez votre chaîne de caractères `texte` en une liste de phrases que vous appelerez `phrases_token`.  \n",
    "- Il y a beaucoup de questions dans cette liste, or on veut des réponses. Supprimez-les.\n",
    "- Récupérez un vecteur de stop words français avec la fonction `get_stop_words` du module `stop_words` \n",
    "\"\"\"\n",
    "f=open('./infos_corona.txt','r',errors = 'ignore', encoding = \"utf8\")\n",
    "texte=f.read()\n",
    "texte = texte.lower()\n",
    "texte = re.sub(r\"\\ufeff\", \" \", texte)\n",
    "texte = re.sub(r\"\\[.{1,2}\\]\", \" \", texte)\n",
    "texte = re.sub(r\"\\n\", \" \", texte)\n",
    "\n",
    "texte = re.sub(' covid-19| virus| covid\\W| sars-cov|ncov', ' coronavirus', texte)\n",
    "texte = re.sub('coronavirus coronavirus', 'coronavirus', texte)\n",
    "texte = re.sub('mort(\\w){0,3}|deces|deced(\\w){1,5}', 'deces', texte)\n",
    "texte = re.sub('medec(\\w){1,5}|medic{1,5}', 'medical', texte) \n",
    "\n",
    "texte = re.sub('c\\'est', 'est', texte)\n",
    "texte = re.sub('/s',' ',texte)\n",
    "#texte = re.sub(f\"[{string.punctuation}]\", \" \", texte)\n",
    "texte = re.sub('[éèê]', 'e', texte)\n",
    "texte = re.sub('[àâ]', 'a', texte)\n",
    "texte = re.sub('[ô]', 'o', texte)\n",
    "\n",
    "phrases_token = nltk.sent_tokenize(texte, language = \"french\")\n",
    "\n",
    "for i in sorted(range(len(phrases_token)), reverse = True):\n",
    "    if re.search(r\"\\?\", phrases_token[i]):\n",
    "        del phrases_token[i]\n",
    "        \n",
    "#phrases_token = list(set(phrases_token))\n",
    "french_stop_words = get_stop_words('french')\n",
    "#texte\n",
    "\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "# un exemple : \n",
    "#LemNormalize(texte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbdf4970-4b4e-431d-a87c-7c80e6289734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#liste = ['avon', 'do', 'doi', 'e', 'fuss', 'no', 'pa', 's', 'sou']\n",
    "french_stop_words = french_stop_words #+ liste\n",
    "tfidf = TfidfVectorizer(stop_words=french_stop_words)\n",
    "phrases_tf = tfidf.fit_transform(phrases_token)\n",
    "\n",
    "\n",
    "def response(phrase_user, tfidf=tfidf, phrases_tf=phrases_tf):\n",
    "    # on a besoin de passer la chaîne de caractère dans une liste :\n",
    "    phrase_user = [phrase_user]\n",
    "    # On calcule les valuers TF-IDF pour la phrase de l'utilisateur\n",
    "    user_tf = tfidf.transform(phrase_user)\n",
    "    # on calcule la similarité entre la question posée par l'utilisateur\n",
    "    # et l'ensemble des phrases de la page wiki\n",
    "    similarity = cosine_similarity(user_tf, phrases_tf).flatten()\n",
    "    # on sort l'index de la phrase étant la plus similaire\n",
    "    index_max_sim = np.argmax(similarity)\n",
    "    # Si la similarité max ets égale à 0 == pas de correspondance trouvée\n",
    "    if (similarity[index_max_sim] == 0):\n",
    "        robo_response = \"Je suis désolé car je ne connais pas la réponse\"\n",
    "    # Sinon, on sort la phrase correspondant le plus : \n",
    "    else:\n",
    "        robo_response = phrases_token[index_max_sim]\n",
    "    return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4662297a-6179-4ce4-a67f-b64b506fc1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"bonjour\", \"salut\")\n",
    "GREETING_RESPONSES = [\"hello\",\"bonjour\", \"je suis ravi de discuter avec vous\"]\n",
    "def greeting(sentence): \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b1713e-4b30-4c29-8b4c-7acb4ea69cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COROBOT: Mon nom est Corobot. Posez moi votre question ou si vous voulez arretez dites simplement bye!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " gestes barrieres\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COROBOT: ce sont les gestes barrieres et la distanciation sociale qui sont efficaces.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " puis je sortir voir des amis ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COROBOT: vous pouvez sortir avec vos enfants ou seul mais pas retrouver des amis.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " faut il porter un masque ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COROBOT: uniquement pour vous occuper de personnes vulnerables comme par exemple les personnes handicapees et en vous rappelant que les seniors sont les personnes les plus vulnerables : il faut les proteger le plus possible de tout contact et porter une attention encore renforcee aux gestes barrieres.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " bonjour\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COROBOT: hello\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " merci\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COROBOT: Avec plaisir.\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"COROBOT: Mon nom est Corobot. Posez moi votre question ou si vous voulez arretez dites simplement bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if re.search(r'bye|à bientôt|au revoir', user_response):\n",
    "        flag=False\n",
    "        print(\"COROBOT: Au revoir, prenez soin de vous et rester chez vous bordel !\")\n",
    "    else:\n",
    "        if(user_response=='merci' or user_response=='merci' ):\n",
    "            flag=False\n",
    "            print(\"COROBOT: Avec plaisir.\")\n",
    "        else:\n",
    "            if greeting(user_response) != None :\n",
    "                print(\"COROBOT: \" + greeting(user_response))\n",
    "            else:\n",
    "                print(\"COROBOT: \" + response(user_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52c658-3621-492f-9285-4042079b02d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
